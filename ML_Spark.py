#!/usr/bin/env python
# coding: utf-8

# In[2]:


'''
Use PySpark to connect to a spark cluster.
 Create a spark session.
 Read a csv file into a data frame.
 Split the dataset into training and testing sets.
 Use VectorAssembler to combine multiple columns into a single vector column
 Use Linear Regression to build a prediction model.
 Use metrics to evaluate the model.
Stop the spark session
'''


# In[4]:


get_ipython().system('pip install pyspark==3.1.2')
get_ipython().system('pip install findspark')


# In[5]:


# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')


# In[15]:


# FindSpark simplifies the process of using Apache Spark with Python

import findspark
findspark.init()

from pyspark.sql import SparkSession

#import functions/Classes for sparkml

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

# import functions/Classes for metrics
from pyspark.ml.evaluation import RegressionEvaluator
import wget


# In[19]:


#Create SparkSession
#Ignore any warnings by SparkSession command

spark = SparkSession.builder.appName("Regressing using SparkML").getOrCreate()


# In[11]:


get_ipython().system('pip install wget')


# In[17]:


wget.download("https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/mpg.csv")


# In[20]:


# using the spark.read.csv function we load the data into a dataframe.
# the header = True mentions that there is a header row in out csv file
# the inferSchema = True, tells spark to automatically find out the data types of the columns.

# Load mpg dataset
mpg_data = spark.read.csv("mpg.csv", header=True, inferSchema=True)


# In[21]:


mpg_data.printSchema()


# In[ ]:


mpg_data.show(5)


# In[ ]:


# Prepare feature vector
assembler = VectorAssembler(inputCols=["Cylinders", "Engine Disp", "Horsepower", "Weight", "Accelerate", "Year"], outputCol="features")
mpg_transformed_data = assembler.transform(mpg_data)


# In[ ]:


mpg_transformed_data.select("features","MPG").show()


# In[ ]:


# Split data into training and testing sets
(training_data, testing_data) = mpg_transformed_data.randomSplit([0.7, 0.3], seed=42)


# In[ ]:


# Train linear regression model
# Ignore any warnings

lr = LinearRegression(featuresCol="features", labelCol="MPG")
model = lr.fit(training_data)


# In[ ]:


# Make predictions on testing data
predictions = model.transform(testing_data)


# In[ ]:


#R-squared (R2): R2 is a statistical measure that represents the proportion of variance
#in the dependent variable (target) that is explained by the independent variables (features).
#Higher values indicate better performance.

evaluator = RegressionEvaluator(labelCol="MPG", predictionCol="prediction", metricName="r2")
r2 = evaluator.evaluate(predictions)
print("R Squared =", r2)


# In[ ]:


#Root Mean Squared Error (RMSE): RMSE is the square root of the average of the squared differences
#between the predicted and actual values. It measures the average distance between the predicted
#and actual values, and lower values indicate better performance.

evaluator = RegressionEvaluator(labelCol="MPG", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("RMSE =", rmse)


# In[ ]:


#Mean Absolute Error (MAE): MAE is the average of the absolute differences between the predicted and
#actual values. It measures the average absolute distance between the predicted and actual values, and
#lower values indicate better performance.

evaluator = RegressionEvaluator(labelCol="MPG", predictionCol="prediction", metricName="mae")
mae = evaluator.evaluate(predictions)
print("MAE =", mae)


# In[ ]:


spark.stop()


# In[24]:


get_ipython().system('java -version')


# In[ ]:


get_ipython().system('pip install --upgrade pyspark')

